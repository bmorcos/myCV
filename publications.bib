@inproceedings{morcos2018,
  author      = {Morcos, Benjamin and Stewart, Terrence C and
                 Eliasmith, Chris and Kapre, Nachiket},
  title       = {Implementing NEF Neural Networks on Embedded FPGAs},
  booktitle   = {International Conference on Field-Programmable
                 Technology (FPT)},
  year        = {2018},
  eventdate   = {2018-12-11/2018-12-14},
  location    = {Naha, Okinawa, Japan},
  publisher   = {IEEE},
  pages       = {25-32},
  doi         = {10.1109/FPT.2018.00015},
  abstract    = {Low-power, high-speed neural networks are critical for
                providing deployable embedded AI applications at the edge. We
                describe an FPGA implementation of Neural Engineering
                Framework (NEF) networks with online learning that outperforms
                mobile GPU implementations by an order of magnitude or more.
                Specifically, we provide an embedded Python-capable PYNQ FPGA
                implementation supported with a High-Level Synthesis (HLS)
                workflow that allows sub-millisecond implementation of
                adaptive neural networks with low-latency, direct I/O access
                to the physical world. We tune the precision of the different
                intermediate variables in the code to achieve competitive
                absolute accuracy against slower and larger floating-point
                reference designs. The online learning component of the neural
                network exploits immediate feedback to adjust the network
                weights to best support a given arithmetic precision. As the
                space of possible design configurations of such networks is
                vast and is subject to a target accuracy constraint, we use
                the Hyperopt hyper-parameter tuning tool instead of manual
                search to find Pareto optimal designs. Specifically, we are
                able to generate the optimized designs in under 500 iterations
                of Vivado HLS before running the complete Vivado
                place-and-route phase on that subset. For neural network
                populations of 64--4096 neurons and 1--8 representational
                dimensions our optimized FPGA implementation generated by
                Hyperopt has a speedup of 10--484$\times$ over a competing
                cuBLAS implementation on the Jetson TX1 GPU  while using
                2.4--9.5$\times$ less power. Our speedups are a result of
                HLS-specific reformulation (15$\times$ improvement), precision
                adaptation (4$\times$ improvement), and low-latency direct I/O
                access (1000$\times$ improvement).},
}

@mathesis{morcos2019,
  author    = {Morcos, Benjamin},
  school    = {University of Waterloo},
  title     = {NengoFPGA: an FPGA Backend for the Nengo Neural Simulator},
  type      = {MASc thesis},
  year      = {2019},
  publisher = "UWSpace",
  url       = {http://hdl.handle.net/10012/14923},
}
